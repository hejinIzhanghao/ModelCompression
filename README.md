# ModelCompression

- Resources
- https://github.com/sun254/awesome-model-compression-and-acceleration
a list of awesome papers on deep model ompression and acceleration
- https://github.com/memoiry/Awesome-model-compression-and-acceleration
a list of awesome papers on deep model ompression and acceleration
- https://github.com/ZhishengWang/Embedded-Neural-Network
collection of works aiming at reducing model sizes or the ASIC/FPGA accelerator for machine learning 

## Tools
- https://pocketflow.github.io/
homepage of pocketflow
- https://github.com/Tencent/PocketFlow
An Automatic Model Compression framework for developing smaller and faster AI applications from Tencent.
- https://github.com/DwangoMediaVillage/keras_compressor
Model Compression CLI Tool for Keras.
- https://github.com/TianzhongSong/Model-Compression-Keras
cnn compression for keras
- https://github.com/walkerning/compression-tool
A small compression tool for caffe model using svd and prune.

## Codes
- https://github.com/chengshengchan/model_compression
Implementation of model compression with three knowledge distilling or teacher student methods. The basic architecture is teacher-student model.
- https://github.com/antspy/quantized_distillation
Implements quantized distillation. Code for our paper "Model compression via distillation and quantization" 
- https://github.com/guoxiaolu/model_compression
keras implementation of "PRUNING FILTERS FOR EFFICIENT CONVNETS"
- https://github.com/Irtza/Keras_model_compression
Model Compression Based on Geoffery Hinton's Logit Regression Method in Keras applied to MNIST 16x compression over 0.95 percent accuracy 
- https://github.com/Roll920/ThiNet
caffe model of ICCV'17 paper - ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression
